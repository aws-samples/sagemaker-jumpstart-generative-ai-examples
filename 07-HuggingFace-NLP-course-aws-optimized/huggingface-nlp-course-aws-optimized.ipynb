{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cba88f5b",
   "metadata": {},
   "source": [
    "### Original Course\n",
    "\n",
    "This course is adapted from the Hugging Face NLP Course to work on Amazon Sagemaker in a workshop environment.\n",
    "\n",
    "If you are interested in seeing the original course, see here:\n",
    "- Original Course:  https://huggingface.co/learn/nlp-course/chapter0/1?fw=pt\n",
    "- Original Course Github: https://github.com/huggingface/course\n",
    "- Original Course Apache 2.0 License: https://github.com/huggingface/course/blob/main/LICENSE\n",
    "\n",
    "Although this repository is released under the MIT-0 license, Models from HuggingFace course use multiple models distilbert-base-uncased-finetuned-sst-2-english, facebook/bart-large-mnli, gpt2, distilgpt2, distilroberta-base, dbmdz/bert-large-cased-finetuned-conll03-english, distilbert-base-cased-distilled-squad, sshleifer/distilbart-cnn-12-6, Helsinki-NLP/opus-mt-fr-en, bert-base-uncased. Models from HuggingFace course are licensed under the Apache 2.0 license."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbb1724-5153-4db5-87f1-c225873d1089",
   "metadata": {},
   "source": [
    "## STEP 0 - ENVIRONMENT SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7da80a86-9516-4396-a82f-7b0c35158b72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.28.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.13.0)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.15.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.0.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: transformers[sentencepiece] in /opt/conda/lib/python3.7/site-packages (4.28.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.7/site-packages (from transformers[sentencepiece]) (0.14.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers[sentencepiece]) (5.4.1)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers[sentencepiece]) (4.13.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers[sentencepiece]) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers[sentencepiece]) (1.21.6)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers[sentencepiece]) (3.0.12)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers[sentencepiece]) (0.13.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers[sentencepiece]) (23.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers[sentencepiece]) (2022.10.31)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers[sentencepiece]) (2.28.2)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /opt/conda/lib/python3.7/site-packages (from transformers[sentencepiece]) (0.1.98)\n",
      "Requirement already satisfied: protobuf<=3.20.2 in /opt/conda/lib/python3.7/site-packages (from transformers[sentencepiece]) (3.20.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers[sentencepiece]) (2023.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers[sentencepiece]) (4.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers[sentencepiece]) (3.15.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers[sentencepiece]) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers[sentencepiece]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers[sentencepiece]) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers[sentencepiece]) (2022.12.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.7/site-packages (2.12.0)\n",
      "Requirement already satisfied: evaluate in /opt/conda/lib/python3.7/site-packages (0.4.0)\n",
      "Requirement already satisfied: transformers[sentencepiece] in /opt/conda/lib/python3.7/site-packages (4.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (5.4.1)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.21.6)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (2023.1.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets) (4.13.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (4.65.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.28.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.14.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers[sentencepiece]) (3.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers[sentencepiece]) (2022.10.31)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers[sentencepiece]) (0.13.3)\n",
      "Requirement already satisfied: protobuf<=3.20.2 in /opt/conda/lib/python3.7/site-packages (from transformers[sentencepiece]) (3.20.2)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /opt/conda/lib/python3.7/site-packages (from transformers[sentencepiece]) (0.1.98)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (0.13.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (22.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (4.5.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.15.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2019.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.14.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: transformers[torch] in /opt/conda/lib/python3.7/site-packages (4.28.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.7/site-packages (from transformers[torch]) (0.14.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers[torch]) (4.65.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers[torch]) (5.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers[torch]) (1.21.6)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers[torch]) (4.13.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers[torch]) (0.13.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers[torch]) (3.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers[torch]) (2022.10.31)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers[torch]) (23.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers[torch]) (2.28.2)\n",
      "Requirement already satisfied: torch!=1.12.0,>=1.9 in /opt/conda/lib/python3.7/site-packages (from transformers[torch]) (1.13.1)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers[torch]) (2023.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers[torch]) (4.5.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.7/site-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.7/site-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.7/site-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.7/site-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (8.5.0.96)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch!=1.12.0,>=1.9->transformers[torch]) (59.3.0)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch!=1.12.0,>=1.9->transformers[torch]) (0.40.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers[torch]) (3.15.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers[torch]) (2.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers[torch]) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers[torch]) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers[torch]) (1.26.15)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (1.13.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.7/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.7/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.7/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.7/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.40.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (59.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install transformers[sentencepiece]\n",
    "!pip install datasets evaluate transformers[sentencepiece]\n",
    "!pip install transformers[torch]\n",
    "!pip install torch\n",
    "# NOTE - ensure you specify the correct instance type [ml.mg4dn.xlarge] using the drop-down in the top-right corner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05ec0ae3-5d43-4fa6-9a15-825d40a0b0eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "from transformers import BertConfig, BertModel\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "import evaluate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56a400f-1573-49d7-97bc-e518d317fc09",
   "metadata": {
    "tags": []
   },
   "source": [
    "## SECTION 1 - Using HuggingFace Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faff8478-5dcc-4882-bd7c-6350b8675dc0",
   "metadata": {},
   "source": [
    "To begin, we want to see what we can do with the Transformers library in Python.\n",
    "\n",
    "Below are a series of examples of different commands you can perform using HuggingFace Transformer functions on the default Generative AI model (distilbert-base-uncased-finetuned-sst-2-english)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6661e303-2a96-4e0a-8df6-0700a136e48f",
   "metadata": {},
   "source": [
    "### Use Case 1 - Sentiment Analysis and Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76e73e14-5c98-4a91-b165-c7c472cbafaa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598050713539124}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the model and model version that we want to use for this use case\n",
    "chosen_model = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "# Example 1 - Perform sentiment analysis\n",
    "classifier = pipeline(task = \"sentiment-analysis\", model = chosen_model)\n",
    "classifier(\"I've been waiting for a HuggingFace course my whole life.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26ed1c66-58bc-4b46-913a-ef237296c381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598050713539124},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 2 - Perform sentiment analysis on multiple input strings\n",
    "classifier(\n",
    "    [\"I've been waiting for a HuggingFace course my whole life.\", \"I hate this so much!\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87ac6ef8-4f98-441b-a1ea-9e50970e7c64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'This is a course about the Transformers library',\n",
       " 'labels': ['education', 'business', 'politics'],\n",
       " 'scores': [0.8445996046066284, 0.1119738295674324, 0.04342665523290634]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the model and model version that we want to use for this use case\n",
    "chosen_model = \"facebook/bart-large-mnli\"\n",
    "\n",
    "# Example 3 - Classify an input based on custom output labels \n",
    "classifier = pipeline(task = \"zero-shot-classification\", model = chosen_model)\n",
    "classifier(\n",
    "    \"This is a course about the Transformers library\",\n",
    "    candidate_labels=[\"education\", \"politics\", \"business\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5f9f9a-2451-494f-83cf-54cb122f8718",
   "metadata": {},
   "source": [
    "### Use Case 2 - Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a697415f-6f56-4ce4-864a-57b368330bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/generation/utils.py:1220: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  \"You have modified the pretrained model configuration to control generation. This is a\"\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/generation/utils.py:1317: UserWarning: Using `max_length`'s default (50) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this course, we will teach you how to identify people who have an issue for which someone has no answer, the way that most people have a conversation about their personal experience and what they feel can hurt and impact their world at large.'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the model and model version that we want to use for this use case\n",
    "chosen_model = \"gpt2\"\n",
    "\n",
    "# Example 4 - Perform ChatGPT-like text generation from a given text prompt\n",
    "generator = pipeline(task = \"text-generation\", model = chosen_model)\n",
    "generator(\"In this course, we will teach you how to\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51bc1a74-dc01-49ec-b291-4d9428846a2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this course, we will teach you how to build software and technologies that will allow us for better collaboration and greater self-management, co-operation'},\n",
       " {'generated_text': 'In this course, we will teach you how to build a beautiful and beautiful art. I hope you will enjoy the course! -MichaelBenson'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the model and model version that we want to use for this use case\n",
    "chosen_model = \"distilgpt2\"\n",
    "\n",
    "# Example 5 - Select a specific mdoel for generating text, constrain the length of repsonses and number of returned sequences\n",
    "generator = pipeline(task = \"text-generation\", chosen_model=\"distilgpt2\")\n",
    "generator(\n",
    "    \"In this course, we will teach you how to\",\n",
    "    max_length=30,\n",
    "    num_return_sequences=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64384088-38d7-47e4-8ec0-7fad1238cf5f",
   "metadata": {},
   "source": [
    "### Use Case 3 - Fill in Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff981e30-10d1-474f-93a8-d4e796f98bc1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.196197971701622,\n",
       "  'token': 30412,\n",
       "  'token_str': ' mathematical',\n",
       "  'sequence': 'This course will teach you all about mathematical models.'},\n",
       " {'score': 0.04052729904651642,\n",
       "  'token': 38163,\n",
       "  'token_str': ' computational',\n",
       "  'sequence': 'This course will teach you all about computational models.'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the model and model version that we want to use for this use case\n",
    "chosen_model = \"distilroberta-base\"\n",
    "\n",
    "# Example 6 - Take a sentence with missing words (indicated by '<mask>'), and determine the top 2 most likely words to fill in that gap\n",
    "unmasker = pipeline(task = \"fill-mask\", model = chosen_model)\n",
    "unmasker(\"This course will teach you all about <mask> models.\", top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c14778-0798-4246-9bed-dd798860b99f",
   "metadata": {},
   "source": [
    "### Use Case 4 - Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa1cb5e9-6469-44dc-b107-09f2b72b8460",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.9981694,\n",
       "  'word': 'Sylvain',\n",
       "  'start': 11,\n",
       "  'end': 18},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9796019,\n",
       "  'word': 'Hugging Face',\n",
       "  'start': 33,\n",
       "  'end': 45},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9932106,\n",
       "  'word': 'Brooklyn',\n",
       "  'start': 49,\n",
       "  'end': 57}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the model and model version that we want to use for this use case\n",
    "chosen_model = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
    "\n",
    "# Example 7 - Identify the key entities (persons, locations or organizations) within a sentence\n",
    "ner = pipeline(task = \"ner\", grouped_entities=True, model = chosen_model)\n",
    "ner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3449d760-350c-4cb2-ae2c-5a646413a751",
   "metadata": {},
   "source": [
    "### Use Case 5 - Answering Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6db148c-9967-4ba6-93ce-ea4e06700ba5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.6949755549430847, 'start': 33, 'end': 45, 'answer': 'Hugging Face'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the model and model version that we want to use for this use case\n",
    "chosen_model = \"distilbert-base-cased-distilled-squad\"\n",
    "\n",
    "# Example 8 - Give a model a context and a question, and produce an answer to the question based on the givne context\n",
    "question_answerer = pipeline(task = \"question-answering\", model = chosen_model)\n",
    "question_answerer(\n",
    "    question=\"Where do I work?\",\n",
    "    context=\"My name is Sylvain and I work at Hugging Face in Brooklyn\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f8059a-a812-4b6e-be81-0727081693dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Use Case 6 - Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d90e960-730b-42d4-bad7-853b901c3114",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' America has changed dramatically during recent years . The number of engineering graduates in the U.S. has declined in traditional engineering disciplines such as mechanical, civil,    electrical, chemical, and aeronautical engineering . Rapidly developing economies such as China and India continue to encourage and advance the teaching of engineering .'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the model and model version that we want to use for this use case\n",
    "chosen_model = \"sshleifer/distilbart-cnn-12-6\"\n",
    "\n",
    "# Example 9 - Summarize a block of text\n",
    "summarizer = pipeline(task = \"summarization\", model = chosen_model)\n",
    "summarizer(\n",
    "    \"\"\"\n",
    "    America has changed dramatically during recent years. Not only has the number of \n",
    "    graduates in traditional engineering disciplines such as mechanical, civil, \n",
    "    electrical, chemical, and aeronautical engineering declined, but in most of \n",
    "    the premier American universities engineering curricula now concentrate on \n",
    "    and encourage largely the study of engineering science. As a result, there \n",
    "    are declining offerings in engineering subjects dealing with infrastructure, \n",
    "    the environment, and related issues, and greater concentration on high \n",
    "    technology subjects, largely supporting increasingly complex scientific \n",
    "    developments. While the latter is important, it should not be at the expense \n",
    "    of more traditional engineering.\n",
    "\n",
    "    Rapidly developing economies such as China and India, as well as other \n",
    "    industrial countries in Europe and Asia, continue to encourage and advance \n",
    "    the teaching of engineering. Both China and India, respectively, graduate \n",
    "    six and eight times as many traditional engineers as does the United States. \n",
    "    Other industrial countries at minimum maintain their output, while America \n",
    "    suffers an increasingly serious decline in the number of engineering graduates \n",
    "    and a lack of well-educated engineers.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a097e0f-29cb-4fbd-ae84-c151990a4556",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Use Case 7 - Language Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc917576-a6b5-4e29-8896-e247927cdb6e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'This course is produced by Hugging Face.'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the model and model version that we want to use for this use case\n",
    "chosen_model = \"Helsinki-NLP/opus-mt-fr-en\"\n",
    "\n",
    "# Example 10 - Translate from French to English\n",
    "translator = pipeline(task = \"translation\", model = chosen_model)\n",
    "translator(\"Ce cours est produit par Hugging Face.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81038762-15d1-40e0-8aa5-f2f95bd9661d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## SECTION 2 - How HuggingFace Transformers Work Under the Hood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e86a17-ca61-465c-8140-0c64e718cb99",
   "metadata": {},
   "source": [
    "Now that we have seen some of the core capabilities of transformers, we will go through a lower-level exploration of how these models work.\n",
    "\n",
    "The first thing to understand about NLP pipelines is that when we make a request, (e.g. for Example 1 in Section 1) using the pipeline() function call, the following steps are completed under-the-hood:\n",
    "1. TOKENIZATION - We take a raw input string (e.g. \"I've been waiting for a HuggingFace course my whole life.\") and translate it into a numeric representation using a process called \"Tokenization\" (e.g. [101, 2023, 2067, 2003, 6429, 999, 102])\n",
    "2. COMPUTATION - Use the model to perform our computation task (e.g. sentiment analysis)\n",
    "3. POST-PROCESSING - Take the mathematical output of the model (e.g. Logit: -4.3630) and translate that into a human-readable format (e.g. POSITIVE: 99.89%)\n",
    "\n",
    "To get a sense of how this is done, we will implement it below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f231328d-74e0-41ca-a904-b5aba5b37a02",
   "metadata": {},
   "source": [
    "### Step 1 - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3900e532-d7e7-4b12-9202-0bca37658862",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set starting checkpoint and select your tokenization model\n",
    "# NOTE - When using Pytorch, you can use 'checkpoints' to save your model as you are training it/working on it\n",
    "model_checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "482bed5d-455f-4cf8-8924-f25db9529d1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "# Tokenize raw inputs (break down the string by word and punctuation point, and convert those bits into integer representations)\n",
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea7d1db-1a45-4265-ad9f-cb0e12df7429",
   "metadata": {},
   "source": [
    "#### Attention Masking \n",
    "Now, it is worth talking briefly about what an 'Attention Mask' is.\n",
    "\n",
    "When you perform inference using a transformer model on multiple strings of text (such as the strings above), we need to pass in sequences that are of the same length.\n",
    "\n",
    "However, the model cannot tell on its own which parts of this matrix are blank.\n",
    "\n",
    "So, for the model to identify which compoenents are blank and which are not, it uses an 'Attention Mask' matrix, where each character in the sequence is represented by a 1 and every blank is identified by 0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbcbc33-164f-4484-96e2-2526159505c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 2 - Invoke the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4848e994-e8a0-4f2c-992c-4c5355f863bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing DistilBertModel: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Update checkpoint\n",
    "model_checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModel.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1cdd5ab4-df0a-4ff3-8850-c731b694fc71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform inference on the model (i.e. generate desired outputs by passing in the inputs\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d16f8df5-f20f-4941-9a52-c4b991b6ba79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16, 768])\n"
     ]
    }
   ],
   "source": [
    "# Inspect model dimensions\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e897326-91fb-41b6-8d75-93d7c3e9a7b7",
   "metadata": {},
   "source": [
    "NOTE - the 'last_hidden-state.shape' command tells us 3 elements about the model\n",
    "1. Batch size - number of sequences processed at a time (2 in our example, due to the 2 sequences of text we passed in)\n",
    "2. Sequence length - Length of the numerical representation (the legnth of the tokenized array passed in - 16 in our example)\n",
    "3. Hidden size - Vector dimension of each model input (i.e. the width of the hidden layer in the underlying neural network we are passing into)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c34d4fe2-7a23-464e-a2cc-3662ad8e87a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "# This performs a similar function, instead it shows the shape of the output values\n",
    "model_checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n",
    "outputs = model(**inputs)\n",
    "print(outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e49a140-6a95-4c51-868c-74f10571309e",
   "metadata": {},
   "source": [
    "In this case, the two dimensions represent:\n",
    "1. Number of output labels (in this case 2 - POSITIVE or NEGATIVE)\n",
    "2. Number of responses (in this case 2 - for the 2 sentences we passed into the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cf6d9b08-1af4-4723-aa77-7072ccf0eeb9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5607,  1.6123],\n",
      "        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# View raw model outputs\n",
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86f7279-b2d8-4b8b-a9de-f0454cc6c885",
   "metadata": {},
   "source": [
    "### Step 3 - Postprocessing the Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cd543a-79e9-466e-af51-cd86a86f6d43",
   "metadata": {},
   "source": [
    "Now that we have the raw output values from the model, we need to translate the final outputs into a human-readable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "edd195f1-19bc-47e2-b2f6-b06e4f795f4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.0195e-02, 9.5981e-01],\n",
      "        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Perform softmax transformation on output variables\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e308eaa7-f4d3-4bc5-88bd-1cfe789b8982",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert model ids to labels\n",
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf02eba8-392b-4359-842c-29565f5a9d39",
   "metadata": {
    "tags": []
   },
   "source": [
    "This tells us that 0 corresponds to a 'Negative' Sentiment and 1 corresponds to a positive sentiment\n",
    "\n",
    "Therefore, we can read the output maxtrix to mean the following:\n",
    "1. First sentence: Probability of Negative Sentiment: 0.0402, Probability of Positive Sentiment: 0.9598\n",
    "2. Second sentence: Probability of Negative Sentiment: 0.99462, Probability of Positive Sentiment: 0.0005 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5529205-5cbc-480a-9acc-434412874363",
   "metadata": {
    "tags": []
   },
   "source": [
    "### SECTION 3 - Fine Tuning a Pre-Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa20f01a-e9ad-4f63-9e6f-ace7fee2f135",
   "metadata": {},
   "source": [
    "Now that we have seen what these models can do and that these models work, we will have a go at fine-tuning a pre-trained language model!\n",
    "\n",
    "To begin with, we will load a dataset that we can use to enhance our pre-trained model with. \n",
    "\n",
    "For the purposes of this workshop, we will download the Microsoft Research Paraphrase Dataset (MRPC - one of the 10 'Glue' academic datasets used to measure model performance) off the HuggingFace dataset repo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88035c52-f8a3-4ac4-8f6e-0d700093d3cc",
   "metadata": {},
   "source": [
    "### Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f5daeb3e-d39c-4b09-b3c5-4871f8d72ddc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15ed73e4d14b45e6ba38ffc85d8eab88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MRPC Dataset\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537fed81-2967-4114-9f0f-2c7f5fa2cecf",
   "metadata": {},
   "source": [
    "As we can see, the dataset is already broken up into train, validation and test sets.\n",
    "\n",
    "On top of that, the dataset contains 4 elements:\n",
    "1. A sentence\n",
    "2. A corresponding sentence that paraphrases the first\n",
    "3. A label for the data\n",
    "4. An index for each set of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9fec0cc7-5f4e-4970-81ad-3af3d7fb895a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
       " 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n",
       " 'label': 1,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example row from training dataset\n",
    "raw_train_dataset = raw_datasets[\"train\"]\n",
    "raw_train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4679d908-9e9d-4ec0-9a04-38ed8104bb55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-734ebcebc978e191.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-a419d1b08f06595c.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-e6b21efb6ccfc370.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that we have the dataset, lets tokenize it\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9e04e5-3a69-4901-b23c-0cc09260f854",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98ecb390-fe71-4a09-96db-7d14a8dca791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we move on to fine-tuning our model.\n",
    "\n",
    "# To begin, we need to create a container for all the hyperparameters the Trainer will use for training and evaluation\n",
    "training_args = TrainingArguments(\"test-trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "381577cb-4fee-4bf0-8a6a-76d34a91455b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Next, we define our model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a807ff3-3996-48ec-8f75-913aaf1bc070",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the parameters we will use for the model training\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f8f78e44-2238-4f75-916f-b2a4c9989ffe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:395: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1377/1377 03:33, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.517100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.305100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1377, training_loss=0.3437755116536813, metrics={'train_runtime': 216.4853, 'train_samples_per_second': 50.83, 'train_steps_per_second': 6.361, 'total_flos': 406183858377360.0, 'train_loss': 0.3437755116536813, 'epoch': 3.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train dataset (this can take a while)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d736b968-1618-4852-a41e-dbeef8a3b552",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5c73be0e-58c1-4a10-b956-e0614b2eed47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(408, 2) (408,)\n"
     ]
    }
   ],
   "source": [
    "# View predictions\n",
    "predictions = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "print(predictions.predictions.shape, predictions.label_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5204e873-f927-44b0-be24-ec2ce807098d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Transform predictions into human readable format\n",
    "preds = np.argmax(predictions.predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "afd9d3b2-92ce-4365-93ca-1eed50d46e0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8504901960784313, 'f1': 0.8967851099830796}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pull standard evaluation metrics for the MRPC dataset to evalaute\n",
    "metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "metric.compute(predictions=preds, references=predictions.label_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b25b02c-ec08-4fcb-a23e-0fb842ccc28e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### How to Interpret Text Model Fine-Tuning\n",
    "Whereas it is easy to see the immediate impact of fine-tuning with image data (you can see the model produce images of things it previously could not), measuring the performance improvement from text fine-tuning can be less obvious.\n",
    "\n",
    "A good starting point is to use industry-standard metrics against industry-standard datasets. For example, for the model we just trained, the model shows an accuracy of ~85.78% and F1 score (a combined measure of precision and accuracy) of ~89.97 (your answers may vary slightly). Given that the original BERT paper only got an f1 score of 88.9, we can say that this dataset improved the LLM response quality for this specific test dataset.\n",
    "\n",
    "However, you will likely also need to consider additional metrics and measurement methods for your specific use case. For example, if you are trying to get your model to identify particular terms in lease documents across multiple state jurisdictions, you might consider calculating F1 scores for each jurisdiction seprartely rather than in aggregate. Furthermore, in this lease document identification, you would need to consider building a training and testing dataset from data specific to your use case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7caac39e-269f-4d92-81d6-20d4e7b43504",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A more complete set of evaluation metrics\n",
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "88c5593c-0768-487c-92c8-28c57ada0d6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# If we want to compute these metrics as part of the training process, we can declare it within the 'trainer' object next time we run it\n",
    "training_args = TrainingArguments(\"test-trainer\", evaluation_strategy=\"epoch\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57597f33-ff36-4bb8-9b7c-917d4f1961e5",
   "metadata": {},
   "source": [
    "NOTE - if you would like to learn more about how to perform model fine-tuning without relying on the Pytorch class (customize it yourself), see this link: https://huggingface.co/learn/nlp-course/chapter3/4?fw=pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc62b110-53d3-4f84-97d8-55ece8369e67",
   "metadata": {
    "tags": []
   },
   "source": [
    "### APPENDIX - Loading and Configuring Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5048b2d4-2d85-4e15-a540-cc6109d138a6",
   "metadata": {},
   "source": [
    "Now that we have seen what the model pipeline looks like under the hood, we will learn about how to load and configure different transformer models.\n",
    "\n",
    "For our purposes, we will use the BERT model from Google."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3113884b-2b58-4981-9e89-2ce095b3faa8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Building the config\n",
    "config = BertConfig()\n",
    "\n",
    "# Building the model from the config\n",
    "model = BertModel(config)\n",
    "\n",
    "# View BERT configuration:\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009c21b3-9b53-4edd-be2e-e53f625a4e76",
   "metadata": {},
   "source": [
    "This describes the different components of the model.\n",
    "\n",
    "While it is beyond the scope of this workshop to explain each element, I recommend reading into each of them further if you are interested.\n",
    "\n",
    "However, what we have done in the previous cell is show that the model is randomly initiallized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c86725b3-3a5a-4ab3-b0cd-c28e4edefbfd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# If we don't want to start with random starting values and instead start\n",
    "# with pre-trained weights, we can do the following:\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "06e9c2b0-4ec9-46c8-9824-79592fa57522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, if we want to save our model as it is in our local directory, then we do the following\n",
    "model.save_pretrained(\"saved_model\")\n",
    "\n",
    "# You will now see a new directory caled 'saved_model' containing your model in your file directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c252de7-9ecf-441b-a630-0ca8a95a9deb",
   "metadata": {},
   "source": [
    "### APPENDIX - Tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b26209-82f5-4f81-b05c-2174e2775076",
   "metadata": {},
   "source": [
    "We talked before about how you need to turn words and sentences into integers through the process of Tokenization for the model to accept the input.\n",
    "\n",
    "There are different approaches you can take to tokenization, including the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c963fdc6-929f-4aec-ba4c-f76e2d8e94f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jim', 'Henson', 'was', 'a', 'puppeteer']\n"
     ]
    }
   ],
   "source": [
    "# Example 1 - Word-based tokenization (splits on spaces)\n",
    "raw_text = \"Jim Henson was a puppeteer\"\n",
    "\n",
    "tokenized_text = raw_text.split()\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1fa97f0d-afbe-497f-a021-1def52f668a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['J', 'i', 'm', ' ', 'H', 'e', 'n', 's', 'o', 'n', ' ', 'w', 'a', 's', ' ', 'a', ' ', 'p', 'u', 'p', 'p', 'e', 't', 'e', 'e', 'r']\n"
     ]
    }
   ],
   "source": [
    "# Example 2 - Character-based tokenization (split on each character)\n",
    "tokenized_text = list(raw_text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba03ed81-a08a-4463-b88e-d28d0b212597",
   "metadata": {},
   "source": [
    "And there are many other methods! including:\n",
    "1. Byte-level BPE, used in GPT-2\n",
    "2. WordPiece, used in BERT\n",
    "3. SentencePiece/Unigram, used in several multilingual models\n",
    "\n",
    "You should chose the tokenization method most appropriate for your resource"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08eeea9d-893e-4bcd-ac77-917b22088df3",
   "metadata": {},
   "source": [
    "### APPENDIX - Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a744b9-8e78-4fa3-8ec3-ee76aaa8f609",
   "metadata": {},
   "source": [
    "As we saw earlier in Section 2, when you pass inputs into a model they need to all be of the same length, even if all the input strings themselves are not the same length on their own.\n",
    "\n",
    "The process of adding adding filler data that the model ignores is called 'padding', and we can see an example of it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aee8b657-aad1-4d06-8dab-63dfcc75101c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Suppose you have a set of 2 inputs of different lengths like so \n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3cbbcf4e-78bc-4597-b441-083450e73750",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We can choose to manually set a value\n",
    "padding_id = 100\n",
    "\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, padding_id],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2a98ea24-80a5-40bc-b198-c3806830fb2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Or we can use a default padding value from the tokenizer function we set earlier\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
