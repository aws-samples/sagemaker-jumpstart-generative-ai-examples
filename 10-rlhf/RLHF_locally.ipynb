{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44a069b0",
   "metadata": {},
   "source": [
    "```\n",
    "Author: Ehsan Kamalinejad (EK)\n",
    "Created: 2023-02-27\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d89c23-8589-48f3-9d97-9312dd4db1a9",
   "metadata": {},
   "source": [
    "- Image: Data Science 2.0\n",
    "- Instance Type: ml.g4dn.xlarge\n",
    "- Kernel: Python 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757937bc",
   "metadata": {},
   "source": [
    "# RLHF\n",
    "\n",
    "A training pipeline to finetune a generative model to create IMDb reviews with positive sentiment according to the [OpenAI RLHF paper](https://arxiv.org/abs/1909.08593)(please see section 3).\n",
    "\n",
    "### Requirements\n",
    "- `tqdm`\n",
    "- `omegaconf`\n",
    "- `dataclasses`\n",
    "- `torchtyping`\n",
    "- `datasets`\n",
    "- `transformers`\n",
    "- `torch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fbe59b2-479e-448e-a508-486e4d078ea9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: jsonschema 3.2.0 does not provide the extra 'format-nongpl'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip --quiet\n",
    "!pip install omegaconf dataclasses torchtyping datasets transformers --quiet\n",
    "!pip install -U accelerate --quiet\n",
    "!pip install ipywidgets==7.0.0 --quiet\n",
    "!pip install xformers --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7f63ee3-e188-4c5b-aca5-6e92b9aa5f1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "806219f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from omegaconf import DictConfig\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, Union\n",
    "from typing import Iterable, Sequence, List\n",
    "\n",
    "from torchtyping import TensorType\n",
    "\n",
    "import transformers\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f63f4904",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'train': {\n",
    "        'seed': 2023,\n",
    "        'seq_length': 1024,\n",
    "        'epochs': 50,\n",
    "        'total_steps': 5000,\n",
    "        'batch_size': 64,\n",
    "        'eval_interval': 100,\n",
    "        'model_device':'cuda:0',\n",
    "        'ref_model_device':'cpu',\n",
    "        'reward_model_device':'cpu'},\n",
    "    'model': {\n",
    "        'model_path': 'lvwerra/gpt2-imdb', #'edbeeching/gpt-neo-1.3B-imdb',\n",
    "        'tokenizer_path': 'lvwerra/gpt2-imdb', #'edbeeching/gpt-neo-1.3B-imdb',\n",
    "        'num_layers_unfrozen': 1},\n",
    "    'optimizer': {\n",
    "        'name': 'adamw',\n",
    "        'kwargs': {'lr': 0.0001,\n",
    "        'betas': [0.9, 0.95],\n",
    "        'eps': 1e-08,\n",
    "        'weight_decay': 1e-06}},\n",
    "    'scheduler': {\n",
    "        'name': 'cosine_annealing',\n",
    "        'kwargs': {\n",
    "            'T_max': 10000, 'eta_min': 0.0001}},\n",
    "    'method': {\n",
    "        'use_whitening': True,\n",
    "        'prompt_size': 10,\n",
    "        'num_rollouts': 128,\n",
    "        'chunk_size': 128,\n",
    "        'ppo_epochs': 4,\n",
    "        'kl_coef': 0.05,\n",
    "        'horizon': 10000,\n",
    "        'gamma': 1,\n",
    "        'lam': 0.95,\n",
    "        'cliprange': 0.2,\n",
    "        'cliprange_value': 0.2,\n",
    "        'vf_coef': 1,\n",
    "        'scale_reward': False,\n",
    "        'ref_mean': None,\n",
    "        'ref_std': None,\n",
    "        'cliprange_reward': 10,\n",
    "        'gen_kwargs': {\n",
    "            'max_new_tokens': 60,\n",
    "            'top_k': 0,\n",
    "            'top_p': 1.0,\n",
    "            'do_sample': True}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf8d10c7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = DictConfig(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a90ff010",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random.seed(config.train.seed)\n",
    "np.random.seed(config.train.seed)\n",
    "torch.manual_seed(config.train.seed)\n",
    "torch.cuda.manual_seed(config.train.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2080a70b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PromptPipeline():\n",
    "    def __init__(self, prompts: List[str], max_prompt_length: int, tokenizer):\n",
    "        super().__init__()\n",
    "\n",
    "        prompts = tokenizer(prompts).input_ids\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.prompts = [prompt[-max_prompt_length:] for prompt in prompts]\n",
    "        self.prompts = [{\"input_ids\": prompt, \"attention_mask\": [1] * len(prompt)} for prompt in self.prompts]\n",
    "\n",
    "    def __getitem__(self, ix: int):\n",
    "        return self.prompts[ix]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.prompts)\n",
    "\n",
    "    def create_loader(self, batch_size: int, shuffle=False) -> DataLoader:\n",
    "        collate_fn = DataCollatorWithPadding(self.tokenizer)\n",
    "        return DataLoader(self, batch_size=batch_size, collate_fn=collate_fn, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e497c3d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PPORLElement:\n",
    "    query_tensor: TensorType[\"query_size\"]\n",
    "    response_tensor: TensorType[\"response_size\"]\n",
    "    logprobs: TensorType[\"response_size\", \"vocab_size\"]\n",
    "    values: TensorType[\"response_size\"]\n",
    "    rewards: TensorType[\"response_size\"]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PPORLBatch:\n",
    "    query_tensors: TensorType[\"batch_size\", \"query_size\"]\n",
    "    response_tensors: TensorType[\"batch_size\", \"response_size\"]\n",
    "    logprobs: TensorType[\"batch_size\", \"response_size\", \"vocab_size\"]\n",
    "    values: TensorType[\"batch_size\", \"response_size\"]\n",
    "    rewards: TensorType[\"batch_size\", \"response_size\"]\n",
    "\n",
    "\n",
    "class PPORolloutStorage():\n",
    "    def __init__(self, pad_token_id):\n",
    "        super().__init__()\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.history: Iterable[PPORLElement] = [None]\n",
    "\n",
    "    def push(self, exps: Iterable[PPORLElement]):\n",
    "        self.history += exps\n",
    "\n",
    "    def clear_history(self):\n",
    "        self.history = []\n",
    "\n",
    "    def __getitem__(self, index: int) -> PPORLElement:\n",
    "        return self.history[index]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.history)\n",
    "\n",
    "    def create_loader(self, batch_size: int, shuffle: bool) -> DataLoader:\n",
    "        def collate_fn(elems: Iterable[PPORLElement]):\n",
    "            return PPORLBatch(\n",
    "                pad_sequence(\n",
    "                    [elem.query_tensor.flip(0) for elem in elems],\n",
    "                    padding_value=self.pad_token_id,\n",
    "                    batch_first=True,\n",
    "                ).flip(1),\n",
    "                pad_sequence(\n",
    "                    [elem.response_tensor for elem in elems],\n",
    "                    padding_value=self.pad_token_id,\n",
    "                    batch_first=True,\n",
    "                ),\n",
    "                pad_sequence(\n",
    "                    [elem.logprobs for elem in elems],\n",
    "                    padding_value=0.0,\n",
    "                    batch_first=True,\n",
    "                ),\n",
    "                pad_sequence(\n",
    "                    [elem.values for elem in elems],\n",
    "                    padding_value=0.0,\n",
    "                    batch_first=True\n",
    "                ),\n",
    "                pad_sequence(\n",
    "                    [elem.rewards for elem in elems],\n",
    "                    padding_value=0.0,\n",
    "                    batch_first=True,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        return DataLoader(self, batch_size, shuffle=shuffle, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d28d570e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def whiten(x):\n",
    "    var, mean = torch.var_mean(x)\n",
    "    return (x - mean) * torch.rsqrt(var + 1e-8)\n",
    "\n",
    "\n",
    "def gae(\n",
    "    values,\n",
    "    rewards,\n",
    "):\n",
    "    advantages = torch.zeros_like(rewards, device=rewards.device)\n",
    "    last_advantage = 0\n",
    "    last_value = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for t in reversed(range(rewards.shape[1])):\n",
    "            delta = rewards[:, t] + config.method.gamma * last_value - values[:, t]\n",
    "            last_advantage = delta + config.method.gamma * config.method.lam * last_advantage\n",
    "            advantages[:, t] = last_advantage\n",
    "            last_value = values[:, t]\n",
    "\n",
    "        returns = advantages + values\n",
    "    \n",
    "    if config.method.use_whitening:\n",
    "        advantages = whiten(advantages)\n",
    "    \n",
    "    return advantages, returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5575874a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ppo_loss(\n",
    "    logprobs,     \n",
    "    values,       \n",
    "    old_logprobs, \n",
    "    old_values,   \n",
    "    advantages,   \n",
    "    returns,      \n",
    "    mask,         \n",
    "):\n",
    "\n",
    "    values_clipped = torch.clamp(\n",
    "        values,\n",
    "        old_values - config.method.cliprange_value,\n",
    "        old_values + config.method.cliprange_value,\n",
    "    )\n",
    "    \n",
    "    n = mask.sum()\n",
    "    \n",
    "    vf_loss1 = (values - returns) ** 2\n",
    "    vf_loss2 = (values_clipped - returns) ** 2\n",
    "    vf_loss = 0.5 * torch.sum(torch.max(vf_loss1, vf_loss2) * mask) / n\n",
    "\n",
    "    log_ratio = (logprobs - old_logprobs) * mask\n",
    "    ratio = torch.exp(log_ratio)\n",
    "    pg_loss1 = -advantages * ratio\n",
    "    pg_loss2 = -advantages * torch.clamp(ratio, 1.0 - config.method.cliprange, 1.0 + config.method.cliprange)\n",
    "    pg_loss = torch.sum(torch.max(pg_loss1, pg_loss2) * mask) / n\n",
    "    pg_clipfrac = torch.sum((pg_loss2 > pg_loss1).float() * mask) / n\n",
    "\n",
    "    loss = pg_loss + config.method.vf_coef * vf_loss\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "811ddf6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_fn(batch):\n",
    "    model_device = next(model.parameters()).device\n",
    "    query_tensors = batch.query_tensors.to(model_device)\n",
    "    response_tensors = batch.response_tensors.to(model_device)\n",
    "    old_logprobs = batch.logprobs.to(model_device)\n",
    "    old_values = batch.values.to(model_device)\n",
    "    old_rewards = batch.rewards.to(model_device)\n",
    "    \n",
    "    response_length = old_rewards.shape[1]\n",
    "\n",
    "    advantages, returns = gae(old_values, old_rewards)\n",
    "\n",
    "    tokens, attention_mask, position_ids = get_model_inputs(query_tensors, response_tensors, tokenizer.pad_token_id)\n",
    "\n",
    "    logits, values_pred = model(tokens,\n",
    "                                attention_mask=attention_mask,\n",
    "                                position_ids=position_ids)\n",
    "    values_pred = values_pred[:, :-1]\n",
    "    logprobs = logprobs_from_logits(logits[:, :-1, :], tokens[:, 1:])\n",
    "    attention_mask = attention_mask[:, :-1]\n",
    "\n",
    "    start = query_tensors.shape[1] - 1\n",
    "    end = start + response_length\n",
    "    logprobs, values_pred, mask = (\n",
    "        logprobs[:, start:end],\n",
    "        values_pred[:, start:end],\n",
    "        attention_mask[:, start:end],\n",
    "    )\n",
    "\n",
    "    loss = ppo_loss(\n",
    "        logprobs=logprobs,\n",
    "        values=values_pred,\n",
    "        old_logprobs=old_logprobs,\n",
    "        old_values=old_values,\n",
    "        advantages=advantages,\n",
    "        returns=returns,\n",
    "        mask=mask,\n",
    "    )\n",
    "\n",
    "    return loss, old_rewards[:,-1].mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f8c337f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Actor():\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            prompt_pipeline,\n",
    "            tokenizer,\n",
    "            chunk_size = 128):\n",
    "        \n",
    "        self.prompt_pipeline = prompt_pipeline\n",
    "        self.chunk_size = chunk_size\n",
    "\n",
    "        self.prompt_pipeline_loader = self.prompt_pipeline.create_loader(self.chunk_size, shuffle=True)\n",
    "        self.prompt_pipeline_iterator = iter(self.prompt_pipeline_loader)\n",
    "\n",
    "        self.ref_model = Agent(config.model.model_path)\n",
    "        self.ref_model_device = config.train.ref_model_device\n",
    "        self.ref_model = self.ref_model.to(self.ref_model_device)\n",
    "        \n",
    "        self.tokenizer = tokenizer        \n",
    "    \n",
    "\n",
    "    def make_experience(self, model, num_rollouts = 128):\n",
    "        model_device = next(model.parameters()).device\n",
    "        \n",
    "        ppo_rl_elements = []\n",
    "        while len(ppo_rl_elements) < num_rollouts:\n",
    "            try:\n",
    "                batch = next(self.prompt_pipeline_iterator)\n",
    "            except StopIteration:\n",
    "                self.pipeline_iterator = iter(self.prompt_pipeline_loader)\n",
    "                batch = next(self.prompt_pipeline_iterator)\n",
    "            \n",
    "            trajectories = generate(model, self.tokenizer, **batch.to(model_device))\n",
    "\n",
    "            query_tensors = batch.input_ids\n",
    "            response_tensors = trajectories[:, query_tensors.shape[1] :]\n",
    "\n",
    "            all_tokens, attention_mask, position_ids = get_model_inputs(\n",
    "                query_tensors.to(response_tensors.device), response_tensors, self.tokenizer.pad_token_id)\n",
    "            with torch.no_grad():\n",
    "                logits, values = model(\n",
    "                    all_tokens, \n",
    "                    attention_mask=attention_mask, \n",
    "                    position_ids=position_ids)\n",
    "                ref_logits, _ = self.ref_model(\n",
    "                    all_tokens.to(self.ref_model_device),\n",
    "                    attention_mask=attention_mask.to(self.ref_model_device),\n",
    "                    position_ids=position_ids.to(self.ref_model_device))\n",
    "            \n",
    "            all_tokens = all_tokens.cpu()\n",
    "            logits = logits.cpu()\n",
    "            ref_logits = ref_logits.cpu()\n",
    "\n",
    "            logprobs = logprobs_from_logits(logits[:, :-1, :], all_tokens[:, 1:])\n",
    "            ref_logprobs = logprobs_from_logits(ref_logits[:, :-1, :], all_tokens[:, 1:])\n",
    "            \n",
    "            n = trajectories.shape[0]\n",
    "            values = values.cpu()[:, :-1]\n",
    "            query_tensors = query_tensors.cpu()\n",
    "            response_tensors = response_tensors.cpu()\n",
    "            \n",
    "            start = query_tensors.shape[1] - 1\n",
    "            ends = start + attention_mask[:, start:].sum(1)\n",
    "            all_values = [values[i, start : ends[i]] for i in range(n)]\n",
    "            all_logprobs = [logprobs[i, start : ends[i]] for i in range(n)]\n",
    "            \n",
    "            texts = self.tokenizer.batch_decode(trajectories, skip_special_tokens=True)\n",
    "            scores = torch.tensor(reward_fn(texts), device='cpu', dtype=torch.float)\n",
    "\n",
    "            rewards = -config.method.kl_coef * (logprobs - ref_logprobs)\n",
    "            all_rewards = [None] * n\n",
    "            for i in range(n):\n",
    "                rs = rewards[i][start : ends[i]]\n",
    "                rs[-1] = scores[i]\n",
    "                all_rewards[i] = rs\n",
    "            \n",
    "            new_ppo_rl_elements = [\n",
    "                PPORLElement(\n",
    "                    query_tensor=query_tensors[i],\n",
    "                    response_tensor=response_tensors[i],\n",
    "                    logprobs=all_logprobs[i],\n",
    "                    values=all_values[i],\n",
    "                    rewards=all_rewards[i],\n",
    "                )\n",
    "                for i in range(n)\n",
    "            ]\n",
    "\n",
    "            ppo_rl_elements += new_ppo_rl_elements\n",
    "\n",
    "        return ppo_rl_elements, scores.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f50e1af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, input_ids, attention_mask=None, **kwargs):\n",
    "    \n",
    "    generate_kwargs = dict(\n",
    "        config.method.gen_kwargs,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    kwargs = dict(generate_kwargs, **kwargs)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_results = model.generate(input_ids=input_ids, attention_mask=attention_mask, **kwargs)\n",
    "\n",
    "    return generated_results\n",
    "\n",
    "\n",
    "def get_model_inputs(query_tensors, response_tensors, pad_token_id):\n",
    "    tokens = torch.cat((query_tensors, response_tensors), dim=1)[:, -config.train.seq_length :]\n",
    "    attention_mask = (tokens.not_equal(pad_token_id).long().to(tokens.device))\n",
    "    position_ids = attention_mask.cumsum(-1) - 1\n",
    "    position_ids.masked_fill_(attention_mask.eq(0), 0)\n",
    "    return tokens, attention_mask, position_ids\n",
    "\n",
    "\n",
    "def logprobs_from_logits(logits, labels):\n",
    "    logprobs = F.log_softmax(logits, dim=-1)\n",
    "    logprobs_labels = torch.gather(logprobs, dim=-1, index=labels.unsqueeze(-1))\n",
    "    return logprobs_labels.squeeze(-1)\n",
    "\n",
    "\n",
    "def freeze_bottom_causal_layers(model: nn.Module, num_layers_unfrozen: int = 0):\n",
    "    hidden_layers = model.transformer.h\n",
    "    if num_layers_unfrozen == 0:\n",
    "        hidden_layers_to_freeze = list(hidden_layers)\n",
    "    elif num_layers_unfrozen > 0:\n",
    "        hidden_layers_to_freeze = list(hidden_layers)[:-num_layers_unfrozen]\n",
    "    else:\n",
    "        hidden_layers_to_freeze = []\n",
    "    for layer in hidden_layers_to_freeze:\n",
    "        layer.requires_grad_(False)\n",
    "\n",
    "        \n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, model_path, num_layers_unfrozen=0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.base_model = transformers.AutoModelForCausalLM.from_pretrained(model_path, cache_dir=\"./models\")\n",
    "\n",
    "        self.logit_head = self.base_model.get_output_embeddings()\n",
    "        \n",
    "        n_embd = self.base_model.lm_head.in_features\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_embd*2, 1))\n",
    "        \n",
    "        freeze_bottom_causal_layers(self.base_model, num_layers_unfrozen)\n",
    "        \n",
    "    \n",
    "    def generate(self, input_ids, **x):\n",
    "        return self.base_model.generate(input_ids, **x)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, position_ids):\n",
    "\n",
    "        transformer_outputs = self.base_model.transformer(input_ids=input_ids,\n",
    "                                                          attention_mask=attention_mask,\n",
    "                                                          position_ids=position_ids)\n",
    "    \n",
    "        last_hidden_state = transformer_outputs.last_hidden_state\n",
    "        lm_logits = self.logit_head(last_hidden_state)\n",
    "        value = self.value_head(last_hidden_state).squeeze(-1)\n",
    "        \n",
    "        return lm_logits, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79835e3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentiment_fn = pipeline(\n",
    "    model = \"lvwerra/distilbert-imdb\",\n",
    "    top_k=2,\n",
    "    batch_size=config.method.num_rollouts,\n",
    "    device=config.train.reward_model_device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e696db6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_positive_score(scores):\n",
    "    return dict(map(lambda x: tuple(x.values()), scores))[\"POSITIVE\"]\n",
    "\n",
    "def reward_fn(samples: List[str]) -> List[float]:\n",
    "    sentiments = list(map(get_positive_score, sentiment_fn(samples)))\n",
    "    return sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "180883d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12e3810fdc0e495287b091194dee4525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cd2dad0a78c466da08efbccc15cc0d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9360cfebf54d4caa82a9c9303318ac70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset imdb/plain_text to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af3ff72a91a54ad08291db682a767d73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset imdb downloaded and prepared to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "imdb = load_dataset(\"imdb\", split=\"train+test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51ad4f7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000,\n",
       " ['I rented I AM CURIOUS-YELLOW from my video store because',\n",
       "  '\"I Am Curious: Yellow\" is a risible and pretentious steaming',\n",
       "  'If only to avoid making this type of film in',\n",
       "  \"This film was probably inspired by Godard's Masculin, féminin and\",\n",
       "  'Oh, brother...after hearing about this ridiculous film for umpteen years'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts = [\" \".join(review.split()[:config.method.prompt_size]) for review in imdb[\"text\"]]\n",
    "len(prompts), prompts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1554f68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd23aff06fb14cd9b42ecac61309a68c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9be6507cfa1a4bd0b8ea1cac8ea1250f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f3ccabf4c3f405dbdcf06ef62b2bbd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5495c044d920463fb83fcf4840566cde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ab2915caa14410da05b12597b10abb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config.model.tokenizer_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_ida = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"left\"\n",
    "pad_token_id = 50256\n",
    "\n",
    "max_prompt_length = (config.train.seq_length - config.method.gen_kwargs[\"max_new_tokens\"])\n",
    "test_prompt_pipeline = PromptPipeline(prompts, max_prompt_length, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "41f0ad62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "384f80fd025a41faa2c8377ea9dc0d69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7225e5839f2c49b98fa4bdf2c5c7cec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Agent(config.model.model_path, config.model.num_layers_unfrozen).to(config.train.model_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c211cb0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1820,  4203,   546,   262,  3807],\n",
       "        [50256, 50256, 50256,  5661,   318],\n",
       "        [   40,   460,  1560,   351, 18979]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer.batch_encode_plus(\n",
    "    [\"my feeling about the movie\", \"this is\", \"I can tell with certainty\"],\n",
    "    return_tensors='pt',\n",
    "    padding=True)['input_ids']\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fbba22bb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_device = next(model.parameters()).device\n",
    "output_ids = generate(model, tokenizer, input_ids.to(model_device), max_new_tokens=config.method.gen_kwargs[\"max_new_tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "284f1b98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my feeling about the movie, as well as the style and tone of the famous \"Amadeus\" music films.',\n",
       " 'this is just awful! it makes you feel very lonely. it is not until next day Italy that you can see epitomise how bad this film was. i was trying to find something im going down the road to television, whilst watching this I felt humiliated, i totally felt embarrassed i have never felt',\n",
       " 'I can tell with certainty that Pulp, Chicago Voice and I like Victor Oppel that he is correct and his Molly Hallrem was one amazing film --a Mustivar of Terror, and a tremendous piece of exceptional film.On well getting past the Great Affair in Vincent Van Buren, Democracy in Love is the']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_text = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1058de6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.988457977771759, 0.006799057591706514, 0.9953449368476868]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_fn(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63696263",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d33813a03e6345f6b9752a44a25d00e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "prompt_pipeline = PromptPipeline(prompts, config.train.seq_length, tokenizer)\n",
    "\n",
    "actor = Actor(prompt_pipeline, tokenizer, chunk_size=config.method.chunk_size)\n",
    "\n",
    "store = PPORolloutStorage(tokenizer.pad_token_id)\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), **config.optimizer.kwargs)\n",
    "scheduler = CosineAnnealingLR(opt, **config.scheduler.kwargs)\n",
    "\n",
    "n_updates_per_batch = config.method.ppo_epochs\n",
    "total_steps = 400 # TODO: fix this\n",
    "\n",
    "tbar = tqdm(initial=0, total=total_steps)\n",
    "\n",
    "for _ in range(config.train.epochs):\n",
    "    \n",
    "    store.clear_history()\n",
    "    rollouts, score = actor.make_experience(model, config.method.num_rollouts)\n",
    "    store.push(rollouts)\n",
    "    train_dataloader = store.create_loader(config.train.batch_size, shuffle=True)\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        for _ in range(n_updates_per_batch):\n",
    "\n",
    "            loss, reward = loss_fn(batch)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            scheduler.step()\n",
    "            tbar.update()\n",
    "    \n",
    "    tbar.set_description(f\"| score: {score:.3f} |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "577f8e73",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1820,  4203,   546,   262,  3807],\n",
       "        [50256, 50256, 50256,  5661,   318],\n",
       "        [   40,   460,  1560,   351, 18979]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer.batch_encode_plus(\n",
    "    [\"my feeling about the movie\", \"this is\", \"I can tell with certainty\"],\n",
    "    return_tensors='pt',\n",
    "    padding=True)['input_ids']\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3af00680",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my feeling about the movie-binding machine was palpable throughout.<br /><br />I am glad I spent more time watching this again, because hopefully I'll watch as many films as I are good at.\n",
      " 0.9671000838279724\n",
      "this is a wonderful film and a gem.\n",
      " 0.9961223006248474\n",
      "I can tell with certainty that this movie has a great deal in common with the Harry Potter trilogy or the Harry Potter films: it's a great treat for kids and adults alike.\n",
      " 0.9945493340492249\n",
      "all rewards mean: 0.9859239061673483\n"
     ]
    }
   ],
   "source": [
    "model_device = next(model.parameters()).device\n",
    "output_ids = generate(\n",
    "    model,       \n",
    "    tokenizer,\n",
    "    input_ids.to(model_device),\n",
    "#     min_length=20,\n",
    "#     max_new_tokens=100,\n",
    "#     do_sample=True,\n",
    "#     top_p=0.92, \n",
    "#     top_k=0\n",
    ")\n",
    "generated_text = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "rewards = reward_fn(generated_text)\n",
    "print(generated_text[0].replace('\\n', ' ') + '\\n', rewards[0])\n",
    "print(generated_text[1].replace('\\n', ' ') + '\\n', rewards[1])\n",
    "print(generated_text[2].replace('\\n', ' ') + '\\n', rewards[2])\n",
    "print('all rewards mean:',np.mean(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a86d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 2.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
